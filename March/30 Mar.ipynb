{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Elastic Net Regression and how does it differ from other regression techniques?\n",
    "\n",
    "Q2. How do you choose the optimal values of the regularization parameters for Elastic Net Regression?\n",
    "\n",
    "Q3. What are the advantages and disadvantages of Elastic Net Regression?\n",
    "\n",
    "Q4. What are some common use cases for Elastic Net Regression?\n",
    "\n",
    "Q5. How do you interpret the coefficients in Elastic Net Regression?\n",
    "\n",
    "Q6. How do you handle missing values when using Elastic Net Regression?\n",
    "\n",
    "Q7. How do you use Elastic Net Regression for feature selection?\n",
    "\n",
    "Q8. How do you pickle and unpickle a trained Elastic Net Regression model in Python?\n",
    "\n",
    "Q9. What is the purpose of pickling a model in machine learning?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is Elastic Net Regression and how does it differ from other regression techniques?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net Regression is a type of linear regression that combines the penalties of L1 (Lasso) and L2 (Ridge) regularization techniques to overcome some of their limitations.\n",
    "\n",
    "Lasso regression shrinks the coefficients of less important features to zero, resulting in feature selection and making it useful when the number of predictors is high. However, it may select only one predictor from a group of highly correlated predictors and may not perform well when the number of predictors is much larger than the number of observations.\n",
    "\n",
    "Ridge regression, on the other hand, can handle correlated predictors but doesn't perform feature selection.\n",
    "\n",
    "Elastic Net Regression combines the penalties of L1 and L2, allowing for both feature selection and handling of correlated predictors. It has two hyperparameters, alpha and lambda, that control the amount of L1 and L2 penalties respectively.\n",
    "\n",
    "By tuning these hyperparameters, Elastic Net Regression can adjust the balance between the two types of regularization and find a model that provides a good tradeoff between bias and variance.\n",
    "\n",
    "Compared to other regression techniques, Elastic Net Regression is more robust to multicollinearity and can handle high-dimensional data with a small sample size. However, it may not perform as well as specialized techniques such as random forests or gradient boosting in some scenarios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. How do you choose the optimal values of the regularization parameters for Elastic Net Regression?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the optimal values of the regularization parameters for Elastic Net Regression requires finding the right balance between bias and variance. This involves tuning the two hyperparameters, alpha and lambda, to achieve the best trade-off between model complexity and prediction accuracy. Here are some common methods to choose optimal values for alpha and lambda:\n",
    "\n",
    "Grid Search: Grid search is a simple and straightforward method that involves evaluating the model's performance on a grid of possible values for alpha and lambda. It involves training the model on different combinations of alpha and lambda values, and choosing the combination that provides the best performance based on a cross-validation approach.\n",
    "\n",
    "Randomized Search: Randomized search is a more efficient alternative to grid search. It randomly selects a set of hyperparameters from a given distribution, and then evaluates the model's performance on those hyperparameters. This method is useful when the hyperparameters space is large.\n",
    "\n",
    "Cross-validation: Cross-validation involves splitting the data into training and validation sets and evaluating the model's performance on the validation set. This process is repeated several times with different validation sets, and the average performance is used as an estimate of the model's performance. This approach can be used in conjunction with grid search or randomized search to find the optimal values of the hyperparameters.\n",
    "\n",
    "Bayesian optimization: Bayesian optimization is a more advanced optimization technique that involves constructing a probabilistic model of the objective function and using it to guide the search for optimal hyperparameters. It has been shown to be an efficient approach for hyperparameter tuning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. What are the advantages and disadvantages of Elastic Net Regression?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of Elastic Net Regression:\n",
    "\n",
    "1. Feature Selection: Elastic Net Regression can perform feature selection by shrinking the coefficients of less important features to zero. This is particularly useful when there are many predictors, and it's not clear which ones are important.\n",
    "\n",
    "2. Handles Correlated Predictors: Elastic Net Regression can handle correlated predictors since it combines the penalties of L1 and L2 regularization.\n",
    "\n",
    "3. Robustness: Elastic Net Regression is more robust to outliers and noise in the data than some other linear regression models, such as the ordinary least squares regression.\n",
    "\n",
    "4. Flexibility: Elastic Net Regression is a flexible model that can handle different types of data, including high-dimensional data with a small sample size.\n",
    "\n",
    "Disadvantages of Elastic Net Regression:\n",
    "\n",
    "1. Selection Bias: Elastic Net Regression can introduce selection bias when there is a large number of predictors, and the sample size is small. This can happen because the model may select predictors based on chance rather than their true importance.\n",
    "\n",
    "2. Hyperparameter Tuning: Elastic Net Regression has two hyperparameters (alpha and lambda) that need to be tuned to achieve optimal performance. This can be a time-consuming and computationally expensive process.\n",
    "\n",
    "3. Interpreting Coefficients: The coefficients estimated by Elastic Net Regression may be difficult to interpret, particularly when there are many predictors and correlations among them.\n",
    "\n",
    "4. Not Suitable for Nonlinear Data: Elastic Net Regression is a linear model, which means it's not suitable for nonlinear data. For nonlinear data, more complex models such as decision trees or neural networks may be more appropriate.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. What are some common use cases for Elastic Net Regression?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net Regression is a popular linear regression technique that can be applied in a wide range of use cases. Here are some common use cases for Elastic Net Regression:\n",
    "\n",
    "1. Predictive Modeling: Elastic Net Regression can be used for predictive modeling tasks, such as predicting customer churn, sales forecasting, or predicting credit risk. In these scenarios, Elastic Net Regression can be used to identify the most important predictors and build a model that provides accurate predictions.\n",
    "\n",
    "2. Feature Selection: Elastic Net Regression can be used for feature selection in machine learning workflows. By identifying the most important features, Elastic Net Regression can help reduce the dimensionality of the data and improve the efficiency and accuracy of downstream modeling tasks.\n",
    "\n",
    "3. Genomics and Proteomics: Elastic Net Regression is commonly used in genomics and proteomics research to identify gene or protein expression patterns associated with diseases or conditions. By selecting the most relevant predictors, Elastic Net Regression can help researchers identify biomarkers and potential drug targets.\n",
    "\n",
    "4. Image and Signal Processing: Elastic Net Regression can be used in image and signal processing applications to identify the most important features or regions of interest. This can be useful for tasks such as object detection, image segmentation, and denoising.\n",
    "\n",
    "5. Financial Analysis: Elastic Net Regression can be used in financial analysis to identify the most important factors that drive stock prices, bond yields, or other financial indicators. By analyzing these factors, analysts can make better predictions about market trends and identify investment opportunities.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. How do you interpret the coefficients in Elastic Net Regression?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the coefficients in Elastic Net Regression can be challenging, especially when there are many predictors and correlations among them. Here are some general guidelines for interpreting the coefficients:\n",
    "\n",
    "Sign: The sign of the coefficient indicates the direction of the relationship between the predictor and the response variable. A positive coefficient indicates that the predictor is positively associated with the response variable, while a negative coefficient indicates a negative association.\n",
    "\n",
    "Magnitude: The magnitude of the coefficient indicates the strength of the relationship between the predictor and the response variable. A larger coefficient indicates a stronger association between the predictor and the response variable, while a smaller coefficient indicates a weaker association.\n",
    "\n",
    "Sparsity: In Elastic Net Regression, some coefficients may be shrunk to zero due to the L1 regularization penalty. This means that some predictors may not have a significant impact on the response variable and can be excluded from the model.\n",
    "\n",
    "Interaction Effects: In some cases, the impact of a predictor on the response variable may depend on the values of other predictors. These interaction effects can be captured by including interaction terms in the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. How do you handle missing values when using Elastic Net Regression?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing values is an important preprocessing step when using Elastic Net Regression, or any other machine learning algorithm. Here are some common approaches for handling missing values in Elastic Net Regression:\n",
    "\n",
    "- Drop rows with missing values: One simple approach is to drop rows that contain missing values. However, this approach can result in a significant loss of data, especially when the percentage of missing values is high.\n",
    "\n",
    "- Imputation: Another approach is to impute the missing values with estimated values. There are various imputation techniques available, such as mean imputation, median imputation, and regression imputation. Mean or median imputation replaces missing values with the mean or median of the available data, while regression imputation uses a regression model to estimate the missing values based on the available data.\n",
    "\n",
    "- Use of missing value indicators: Another approach is to include a separate indicator variable for each predictor with missing values. This indicator variable takes the value of 1 if the corresponding predictor is missing, and 0 otherwise. This allows the model to distinguish between missing values and actual values and can help preserve the information contained in the available data.\n",
    "\n",
    "- Model-based imputation: Model-based imputation methods, such as multiple imputation, involve creating multiple imputed datasets based on statistical models, then combining the results using appropriate rules. These methods can provide more accurate estimates and better account for uncertainty in the imputed values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. How do you use Elastic Net Regression for feature selection?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net Regression is a useful technique for feature selection, as it allows for simultaneous shrinkage and selection of predictors through the use of the L1 regularization penalty. Here are some general steps for using Elastic Net Regression for feature selection:\n",
    "\n",
    "- Prepare the data: Prepare the data by selecting the predictors and the response variable, and handle any missing values or outliers.\n",
    "\n",
    "- Standardize the data: Standardize the predictors to have zero mean and unit variance. This is important because the regularization penalty in Elastic Net Regression is sensitive to the scale of the predictors.\n",
    "\n",
    "- Fit the Elastic Net Regression model: Fit the Elastic Net Regression model using the standardized data. The model should be fit with different values of the regularization parameters, alpha and lambda, to find the optimal values.\n",
    "\n",
    "- Determine the important predictors: Identify the important predictors by examining the magnitude and sign of the coefficients. Predictors with large, nonzero coefficients are considered important predictors.\n",
    "\n",
    "- Evaluate the model: Evaluate the performance of the Elastic Net Regression model using the selected predictors. This can be done by calculating the prediction error on a holdout dataset or using cross-validation.\n",
    "\n",
    "- Refine the model: If necessary, refine the model by adding or removing predictors, adjusting the regularization parameters, or using a different model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. How do you pickle and unpickle a trained Elastic Net Regression model in Python?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4m/vrm5g1l51xn5x3bdqm97xhsr0000gn/T/ipykernel_87808/1211810661.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElasticNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Pickle the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mX_copied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m             X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'ellipsis'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Train the model\n",
    "X_train = ...\n",
    "y_train = ...\n",
    "model = ElasticNet(alpha=0.5, l1_ratio=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Pickle the model\n",
    "filename = 'elastic_net_model.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Unpickle the model\n",
    "with open(filename, 'rb') as file:\n",
    "    unpickled_model = pickle.load(file)\n",
    "\n",
    "# Use the unpickled model for prediction\n",
    "X_test = ...\n",
    "y_pred = unpickled_model.predict(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. What is the purpose of pickling a model in machine learning?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of pickling a model in machine learning is to save a trained model so that it can be reused or shared with others without having to retrain it from scratch. Pickling allows you to serialize a trained model object into a file, which can then be loaded back into memory later on and used to make predictions on new data.\n",
    "\n",
    "Pickling a model can be particularly useful in the following scenarios:\n",
    "\n",
    "- Deploying a machine learning model: Once a machine learning model has been trained and tested, it can be pickled and deployed in a production environment. This can save time and computing resources since the model doesn't need to be retrained every time it needs to make predictions.\n",
    "\n",
    "- Collaborating with others: When working on a machine learning project with others, pickling the trained model can make it easy to share the model with other team members. This allows other team members to use the model for their own analyses or to build on it further.\n",
    "\n",
    "- Saving intermediate results: During the process of building a machine learning model, it can be useful to save intermediate results, such as a trained model, for future use. Pickling the model allows you to save the model in a serialized format, which can be loaded back into memory later on and used to continue model development or analysis.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
