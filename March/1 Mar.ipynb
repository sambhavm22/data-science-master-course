{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f69ace9-9485-4d2f-a7e1-b9982c344232",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1503f2c5-4397-4f8a-8650-3f73c1ffc21d",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52202b7c-3feb-4078-b528-5e6677f2a2a7",
   "metadata": {},
   "source": [
    "Min-Max scaling is a technique used in data preprocessing to scale numerical features to a specific range, usually between 0 and 1 or -1 and 1. The goal is to normalize the data so that features with large values do not dominate the learning algorithm, which can cause issues with the model's performance. \n",
    "\n",
    "Min-Max formula = (xi - x_min) / (x_max - x_min) #### for range (0,1)\n",
    "\n",
    "Min-Max formula = (xi - x_min) / (x_max - x_min) * 2 - 1 #### for range (-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3eb066b-d659-4731-9a73-45b3c0fd8997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.270833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.210526</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age    income\n",
       "0  0.000000  0.000000\n",
       "1  0.815789  0.583333\n",
       "2  0.289474  0.166667\n",
       "3  0.605263  0.458333\n",
       "4  1.000000  0.270833\n",
       "5  0.210526  1.000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "d = {'age': [23,54,34,46,61,31],\n",
    "     'income' : [20000, 300000, 100000, 240000, 150000, 500000]}\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "min_max = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "scaled_df = min_max.fit_transform(df)\n",
    "\n",
    "pd.DataFrame(scaled_df, columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceebcff-e621-4599-8a11-707f2364cb16",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82b51eb-9b2b-4351-a850-3fcda60599fd",
   "metadata": {},
   "source": [
    "Unit Vector scaling is a technique used in data preprocessing to scale numerical features such that each feature vector has a length of 1. This is achieved by dividing each feature value by the Euclidean norm of the feature vector, which is the square root of the sum of the squares of each feature value.\n",
    "\n",
    "The main difference between Unit Vector scaling and Min-Max scaling is that Unit Vector scaling preserves the direction of the feature vectors, while Min-Max scaling only preserves the relative distances between the feature vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb49ba2b-7e60-4738-b2f5-e4ff14797114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000180</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000340</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000192</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000407</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000062</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age    income\n",
       "0  0.001150  0.999999\n",
       "1  0.000180  1.000000\n",
       "2  0.000340  1.000000\n",
       "3  0.000192  1.000000\n",
       "4  0.000407  1.000000\n",
       "5  0.000062  1.000000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "d = {'age': [23,54,34,46,61,31],\n",
    "     'income' : [20000, 300000, 100000, 240000, 150000, 500000]}\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "unit_vector = Normalizer()\n",
    "\n",
    "scaled_uni_vector = unit_vector.fit_transform(df)\n",
    "\n",
    "scaled_df_1 = pd.DataFrame(scaled_uni_vector, columns=df.columns)\n",
    "\n",
    "scaled_df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de96f63d-2f98-4a9b-8623-037586b6d082",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3c6db-8263-4069-84d9-480285589c68",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a widely used technique for dimensionality reduction, which involves transforming a large set of variables into a smaller set of uncorrelated variables called principal components.\n",
    "\n",
    "PCA works by identifying the directions in the feature space where the variance of the data is maximum and then projecting the data onto those directions. The resulting principal components are ordered in terms of their explained variance, where the first principal component explains the most variance in the data, and each subsequent component explains progressively less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61cebc10-6b80-4c66-ae12-2aabf956efa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.68412563  0.31939725]\n",
      " [-2.71414169 -0.17700123]\n",
      " [-2.88899057 -0.14494943]\n",
      " [-2.74534286 -0.31829898]\n",
      " [-2.72871654  0.32675451]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# create an instance of the PCA class with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# fit and transform the data\n",
    "iris_pca = pca.fit_transform(iris.data)\n",
    "\n",
    "print(iris_pca[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f186b2-81cf-4549-9ef2-5992b7fc3cd6",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa548b9-0b67-4975-a100-454e632ac962",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a technique for feature extraction, which involves transforming a large set of variables into a smaller set of uncorrelated variables called principal components. Feature extraction is the process of reducing the dimensionality of the data by selecting a subset of the original features.\n",
    "\n",
    "PCA can be used for feature extraction by identifying the directions in the feature space where the variance of the data is maximum and then projecting the data onto those directions. The resulting principal components are ordered in terms of their explained variance, where the first principal component explains the most variance in the data, and each subsequent component explains progressively less. By selecting a subset of the principal components, we can effectively reduce the dimensionality of the data while retaining most of the important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d27113-ecee-4198-92a8-0318512ba8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ -1.25944953,  21.27481525,  -9.46294903, ...,   2.56424314,\n",
       "          -0.58727595,   3.60167822],\n",
       "        [  7.95759679, -20.76882077,   4.43939022, ...,  -4.61145426,\n",
       "           3.53649103,  -1.02412114],\n",
       "        [  6.99187905,  -9.95584018,   2.95870928, ..., -16.41198281,\n",
       "           0.75892723,   4.28081477],\n",
       "        ...,\n",
       "        [ 10.80126114,  -6.96000443,   5.59951508, ...,  -7.43308853,\n",
       "          -3.89862392, -13.0739433 ],\n",
       "        [ -4.8721472 ,  12.42395304, -10.1707731 , ...,  -4.33296756,\n",
       "           3.91927311, -13.08721077],\n",
       "        [ -0.34438531,   6.36568448,  10.77377433, ...,   0.66309635,\n",
       "          -4.06781981, -12.59296193]]),\n",
       " (1797, 10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "digit_pca = pca.fit_transform(digits.data)\n",
    "\n",
    "digit_pca, digit_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316437b-87a6-4afb-9ee7-44b66f79a477",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb536240-2187-4a7f-8cf9-70201b336720",
   "metadata": {},
   "source": [
    "We have a dataset of food delivery orders that includes the following features:\n",
    "\n",
    "Price (in dollars)\n",
    "Rating (on a scale of 1 to 5)\n",
    "Delivery time (in minutes)\n",
    "We could apply Min-Max scaling to each of these features so that they fall within the range of 0 to 1. This would involve subtracting the minimum value of each feature from each data point, dividing the result by the range of the feature, and then multiplying by the desired range (in this case, 1). This would result in a dataset where each feature has been normalized to the same scale, making it easier to compare and analyze the data.\n",
    "\n",
    "For example, the Min-Max scaling formula for the price feature would be:\n",
    "\n",
    "scaled_price = (price - min_price) / (max_price - min_price) * 1\n",
    "\n",
    "where min_price is the minimum price in the dataset, max_price is the maximum price in the dataset, and scaled_price is the transformed price value betw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82d97a86-8f85-4a83-bbcb-4889e4c5427f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>delivery time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  rating  delivery time\n",
       "0  1.000000  0.0000            0.5\n",
       "1  0.000000  0.4375            1.0\n",
       "2  0.200000  1.0000            0.3\n",
       "3  0.666667  0.2500            0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = {'price' : [299, 149, 179, 249], \n",
    "       'rating' : [3.4, 4.1, 5.0, 3.8], \n",
    "       'delivery time' : [20, 25, 18, 15]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "min_max = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "scaled = min_max.fit_transform(df)\n",
    "\n",
    "scaled_df = pd.DataFrame(scaled, columns=df.columns)\n",
    "\n",
    "scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6eec8ce9-b75b-4f6f-9f88-8c78ab193ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.5       ],\n",
       "       [0.        , 0.4375    , 1.        ],\n",
       "       [0.2       , 1.        , 0.3       ],\n",
       "       [0.66666667, 0.25      , 0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X = np.array([[299, 3.4, 20], [149,4.1,25], [179,5.0, 18], [249, 3.8, 15]])\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "transformed_value = scaler.fit_transform(X)\n",
    "\n",
    "transformed_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e9ae44-c3e1-4457-baf5-fff505bdbcc6",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75e231-ea68-4430-8464-ef86f4b0953d",
   "metadata": {},
   "source": [
    "Standardize the data: First, we would need to standardize the data by subtracting the mean and dividing by the standard deviation of each feature. This ensures that each feature has the same scale and allows for more effective PCA analysis.\n",
    "\n",
    "Determine the number of principal components: We would need to determine the number of principal components to retain for our analysis. This can be done by analyzing the explained variance of each principal component and choosing the number that explains most of the variance in the data.\n",
    "\n",
    "Perform PCA: We would perform PCA to transform the data into a new set of principal components. Each principal component is a linear combination of the original features, with the first principal component explaining the most variance in the data.\n",
    "\n",
    "Choose the new features: We would choose the new features based on the most important principal components. These features are the linear combination of the original features that contribute the most to the variance in the data.\n",
    "\n",
    "Train the model: We would then train the model on the reduced feature set and evaluate its performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff50b6-e711-4ba3-9e17-bdf55c5ed8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "X = ...\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=5)\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "\n",
    "# Get the explained variance of each principal component\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "# Choose the new features based on the most important principal components\n",
    "new_features = X_pca[:, :2]\n",
    "\n",
    "# Train the model on the reduced feature set\n",
    "model = ...\n",
    "model.fit(new_features, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a5888a-3871-403e-bb29-aa354c77aa64",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39ba0141-5f50-4152-b7ad-88be7744e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = [[1],[5],[10],[15],[20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5abd9e2c-081e-4378-b9b1-74ae188c8c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler(feature_range=(-1, 1))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler(feature_range=(-1, 1))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler(feature_range=(-1, 1))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max = MinMaxScaler(feature_range=(-1,1))\n",
    "min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c55eaeb8-dedf-4b1d-b4a2-67cf5e5c949d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03578de9-eeec-476f-85db-594f8c958db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler(feature_range=(-1, 1))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler(feature_range=(-1, 1))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler(feature_range=(-1, 1))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1570aa10-4148-437b-9a70-5566cf4e7791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4cb4af4-69e5-49c6-be67-c70c310bafc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]\n"
     ]
    }
   ],
   "source": [
    "x = [1, 5, 10, 15, 20]\n",
    "\n",
    "# calculate the minimum and maximum values\n",
    "x_min = min(x)\n",
    "x_max = max(x)\n",
    "\n",
    "# perform Min-Max scaling\n",
    "x_scaled = [(xi - x_min) / (x_max - x_min) * 2 - 1 for xi in x]\n",
    "\n",
    "print(x_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394c8964-8e04-44c4-a7e1-05bba675f740",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2f7cf6-fd68-4289-96fd-96a5e37648ea",
   "metadata": {},
   "source": [
    "The number of principal components to retain would depend on the specific requirements of the project, the amount of variance in the data that we want to retain, and the trade-off between accuracy and computational efficiency.\n",
    "\n",
    "To determine the number of principal components to retain, we can use the scree plot or the cumulative explained variance plot. The scree plot shows the amount of variance explained by each principal component, while the cumulative explained variance plot shows the cumulative amount of variance explained as a function of the number of principal components. We can select the number of principal components that explain a significant amount of variance in the data, such as 80% or 90%, and ignore the rest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4986c1d9-0de7-41f6-8ef9-48cb7e6bce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('health_data.csv')\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = df[['height', 'weight', 'age', 'gender', 'blood_pressure']]\n",
    "y = df['target']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Check the explained variance ratio of the principal components\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Retain the first three principal components\n",
    "X_pca = X_pca[:, :3]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
