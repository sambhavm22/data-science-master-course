{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning algorithm that belongs to the ensemble learning family. It is a powerful technique used for regression tasks, which involves predicting continuous numerical values.\n",
    "\n",
    "Gradient Boosting Regression builds an ensemble of weak regression models, typically decision trees, in a sequential manner. The algorithm combines the predictions of these weak models to create a strong predictive model that can accurately estimate the target variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.base_prediction = np.mean(y)\n",
    "        self.models.append(self.base_prediction)  # Initialize with base prediction\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            residual = y - self.predict(X)  # Calculate residuals\n",
    "            \n",
    "            # Train a weak learner on the residuals\n",
    "            model = DecisionTreeRegressor(max_depth=1)\n",
    "            model.fit(X, residual)\n",
    "            \n",
    "            # Update the ensemble\n",
    "            self.models.append(model)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # Make predictions by summing up the predictions of all models\n",
    "        predictions = np.array([model.predict(X) for model in self.models[1:]])  # Exclude the base prediction\n",
    "        return self.base_prediction + self.learning_rate * np.sum(predictions, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the gradient boosting model\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.5591548081475488\n",
      "R-squared: 0.9988817056997036\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Gradient Boosting model\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gb, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "Mean Squared Error: 1.3379888778506104\n",
      "R-squared: 0.9990403356176427\n"
     ]
    }
   ],
   "source": [
    "# Train the best model\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the best model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weak learner refers to a simple, relatively low-complexity model that performs slightly better than random guessing on a given learning task. It is also known as a base learner or a weak predictor. Weak learners are typically used as building blocks in the ensemble of models created by Gradient Boosting algorithms.\n",
    "\n",
    "The concept of a weak learner is central to the working of Gradient Boosting algorithms, such as AdaBoost and Gradient Boosting Machines (GBM). These algorithms aim to combine multiple weak learners to create a strong learner that can make accurate predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ensemble Learning: Gradient Boosting is an ensemble learning technique that combines multiple weak learners (typically decision trees) to create a strong learner. The idea is to leverage the collective knowledge and predictive power of these weak learners to make accurate predictions.\n",
    "\n",
    "2. Sequential Training: The weak learners are trained sequentially, with each subsequent learner attempting to correct the errors or residuals made by the previous learners. The algorithm iteratively adds new learners and adjusts their predictions to minimize the overall prediction error.\n",
    "\n",
    "3. Gradient Descent Optimization: The term \"gradient\" in Gradient Boosting refers to the technique of using gradient descent optimization to minimize the loss function. The algorithm minimizes the loss by finding the negative gradient (direction of steepest descent) of the loss function with respect to the predicted values.\n",
    "\n",
    "4. Residual-based Learning: The subsequent weak learners are trained to predict the residuals or errors made by the previous learners. This approach allows the algorithm to focus on the areas where the previous learners performed poorly, gradually improving the overall predictions.\n",
    "\n",
    "5. Weighted Contributions: Each weak learner's prediction is assigned a weight or learning rate, which determines its contribution to the final prediction. The learning rate controls the step size of each iteration and can be adjusted to balance between overfitting and underfitting.\n",
    "\n",
    "6. Ensemble Combination: The final prediction is obtained by combining the predictions of all weak learners, often using a weighted average. The weighting considers the individual contributions of each learner based on their performance and learning rate.\n",
    "\n",
    "7. Regularization: Gradient Boosting employs regularization techniques to prevent overfitting. Regularization parameters, such as tree depth, learning rate, and subsampling, can be tuned to control the complexity of the model and improve generalization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize the ensemble: The algorithm starts by initializing the ensemble with a simple model, which serves as the initial prediction. This can be a constant value or the mean of the target variable.\n",
    "\n",
    "2. Calculate the residual errors: The algorithm calculates the residual errors between the actual target values and the predictions made by the current ensemble. The residuals represent the information that the current ensemble has not yet captured.\n",
    "\n",
    "3. Train a weak learner: A weak learner (e.g., decision tree) is trained on the dataset, focusing on predicting the residual errors. The goal is to find a weak learner that can capture the patterns in the residuals and improve the predictions of the ensemble.\n",
    "\n",
    "4. Update the ensemble: The weak learner's predictions are added to the ensemble by multiplying them with a learning rate, which determines their contribution to the final prediction. The learning rate acts as a regularization parameter to control the update step size.\n",
    "\n",
    "5. Update the residual errors: The algorithm updates the residual errors by subtracting the weak learner's predictions from the previous residuals. This focuses the subsequent weak learners on the remaining errors that need to be addressed.\n",
    "\n",
    "6. Repeat steps 3-5: Steps 3-5 are repeated for a specified number of iterations or until a certain stopping criterion is met. In each iteration, a new weak learner is trained on the updated residuals, and the ensemble is updated.\n",
    "\n",
    "7. Final prediction: The final prediction is obtained by summing up the predictions of all weak learners in the ensemble, possibly weighted by their learning rates. The ensemble's combined predictions provide a more accurate estimate of the target variable compared to individual weak learners.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the Loss Function: Start by defining a differentiable loss function that measures the discrepancy between the predicted values and the actual target values. Common examples include mean squared error (MSE) for regression problems and log loss or exponential loss for classification problems.\n",
    "\n",
    "2. Initialize the Ensemble: Initialize the ensemble by assigning initial predictions to be used as the starting point. For regression, this could be a constant value or the mean of the target variable. For classification, it could be the log odds or the class probabilities.\n",
    "\n",
    "3. Compute the Negative Gradient: Calculate the negative gradient of the loss function with respect to the initial predictions. This represents the direction of steepest descent or the amount by which the predictions need to be adjusted to minimize the loss.\n",
    "\n",
    "4. Train a Weak Learner: Train a weak learner, typically a decision tree, on the negative gradient. The weak learner is trained to approximate the negative gradient and make predictions that move in the direction of minimizing the loss.\n",
    "\n",
    "5. Update the Ensemble: Update the ensemble by adding the predictions of the weak learner, multiplied by a learning rate or shrinkage parameter. The learning rate controls the contribution of each weak learner to the final prediction and prevents overfitting.\n",
    "\n",
    "6. Update the Residuals: Calculate the new residuals by subtracting the predictions made by the weak learner from the previous residuals. The residuals represent the information that the ensemble has not yet captured and need to be further addressed.\n",
    "\n",
    "7. Repeat Steps 4-6: Repeat steps 4-6 for a specified number of iterations or until a stopping criterion is met. In each iteration, a new weak learner is trained on the updated residuals, and the ensemble is updated accordingly.\n",
    "\n",
    "8. Final Prediction: Obtain the final prediction by summing up the predictions of all weak learners in the ensemble, possibly weighted by their learning rates. This provides the final prediction that minimizes the loss function.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
