{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "\n",
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "Q5. Assignment:\n",
    "\n",
    "- Import the necessary libraries and load the dataset\n",
    "- Split the dataset into training and testing sets. \n",
    "- Preprocess the data using any technique of your choice (e.g. scaling, normalization)\n",
    "- Create an instance of the SVC classifier and train it on the training data\n",
    "- hse the trained classifier to predict the labels of the testing data\n",
    "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-score)\n",
    "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performance\n",
    "- Train the tuned classifier on the entire dataset\n",
    "- Save the trained classifier to a file for future use."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial functions and kernel functions are closely related in machine learning algorithms, particularly in kernel methods such as Support Vector Machines (SVMs). The relationship lies in the concept of the kernel trick.\n",
    "\n",
    "In SVMs, the kernel trick is a mathematical technique that allows us to implicitly map the input data into a higher-dimensional feature space without explicitly calculating the transformed features. It avoids the need for explicit feature mapping, which can be computationally expensive for high-dimensional data.\n",
    "\n",
    "Polynomial functions are one type of kernel function commonly used in SVMs. A polynomial kernel computes the similarity between two data points in the original input space by evaluating the polynomial function of their dot product. The polynomial kernel function has the form:\n",
    "\n",
    "K(x, y) = (gamma * <x, y> + coef0)^degree\n",
    "\n",
    "where gamma, coef0, and degree are hyperparameters that control the shape and flexibility of the decision boundary. The dot product <x, y> is calculated in the original input space.\n",
    "\n",
    "By using the polynomial kernel, the SVM can implicitly perform computations in a higher-dimensional feature space without explicitly mapping the data to that space. This allows SVMs to learn complex nonlinear decision boundaries.\n",
    "\n",
    "Overall, polynomial functions are a type of kernel function used in SVMs and other kernel-based machine learning algorithms to enable efficient and flexible learning in higher-dimensional feature spaces without explicitly computing the transformed features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "svm = SVC(kernel='poly', degree=3)  # degree is the degree of the polynomial kernel\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), the epsilon parameter, often denoted as ε, controls the width of the epsilon-insensitive zone around the predicted values. It determines the tolerance for errors that are considered acceptable.\n",
    "\n",
    "Increasing the value of epsilon in SVR will typically lead to an increase in the number of support vectors. This is because a larger epsilon allows more training instances to fall within the epsilon-insensitive zone without violating the margin.\n",
    "\n",
    "When epsilon is small, the SVR model aims to fit the training data more precisely, resulting in a narrower epsilon-insensitive zone. This can lead to a smaller number of support vectors, as the model is more focused on minimizing errors directly on or very close to the data points.\n",
    "\n",
    "On the other hand, when epsilon is larger, the SVR model allows more training instances to be within the epsilon-insensitive zone, even if they have larger deviations from the predicted values. This broader tolerance for errors can result in a larger number of support vectors, as the model is more lenient and accepts a wider range of instances within the epsilon band.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Kernel Function:\n",
    "\n",
    "The kernel function determines the type of mapping used to transform the input data into a higher-dimensional feature space. Different kernel functions capture different types of relationships in the data. Some commonly used kernel functions in SVR are linear, polynomial, Gaussian (RBF), and sigmoid.\n",
    "\n",
    "- Linear Kernel: Suitable for linear relationships in the data.\n",
    "- Polynomial Kernel: Captures polynomial relationships between features.\n",
    "- Gaussian (RBF) Kernel: Useful for capturing complex, non-linear relationships.\n",
    "- Sigmoid Kernel: Can model non-linear relationships with a sigmoidal shape.\n",
    "\n",
    "The choice of the kernel function depends on the specific characteristics of the data and the underlying relationship you want to capture. It may require experimentation and cross-validation to determine the most suitable kernel function.\n",
    "\n",
    "2. C Parameter:\n",
    "\n",
    "The C parameter controls the trade-off between minimizing the training error and the complexity of the decision function. It represents the regularization parameter in SVR. A larger C value allows for a more complex model with a narrower margin, potentially leading to overfitting. Conversely, a smaller C value enforces a larger margin and a simpler model, which may result in underfitting.\n",
    "\n",
    "- Increase C: May lead to overfitting but can provide a better fit to the training data.\n",
    "- Decrease C: Can prevent overfitting, but the model may not capture all the complexities in the data.\n",
    "\n",
    "3. Epsilon Parameter:\n",
    "\n",
    "The epsilon parameter, often denoted as ε, defines the width of the epsilon-insensitive zone around the predicted values. It determines the tolerance for errors that are considered acceptable. Instances falling within this zone are not considered errors and do not contribute to the loss function.\n",
    "\n",
    "- Increase Epsilon: Expands the epsilon-insensitive zone, allowing more training instances to be within the zone, potentially increasing the number of support vectors and increasing model complexity.\n",
    "- Decrease Epsilon: Narrows the epsilon-insensitive zone, making the model more focused on fitting the training data precisely, potentially reducing the number of support vectors and simplifying the model.\n",
    "\n",
    "4. Gamma Parameter:\n",
    "\n",
    "The gamma parameter determines the influence of a single training example and controls the shape of the decision boundary. It defines the reach of each training example in the feature space.\n",
    "\n",
    "- Increase Gamma: Leads to a more localized and complex decision boundary. It may result in overfitting if the value is too high.\n",
    "- Decrease Gamma: Expands the influence of each training example, potentially leading to a smoother decision boundary. It can help prevent overfitting, especially when dealing with large datasets.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "\n",
    "- Import the necessary libraries and load the dataset\n",
    "- Split the dataset into training and testing sets. \n",
    "- Preprocess the data using any technique of your choice (e.g. scaling, normalization)\n",
    "- Create an instance of the SVC classifier and train it on the training data\n",
    "- hse the trained classifier to predict the labels of the testing data\n",
    "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-score)\n",
    "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performance\n",
    "- Train the tuned classifier on the entire dataset\n",
    "- Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides   \n",
       "0               7.4             0.700         0.00             1.9      0.076  \\\n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates   \n",
       "0                    11.0                  34.0  0.99780  3.51       0.56  \\\n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/aakanksha/My_Codes/data-science-master-course/data/winequality-red.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity           0\n",
       "volatile acidity        0\n",
       "citric acid             0\n",
       "residual sugar          0\n",
       "chlorides               0\n",
       "free sulfur dioxide     0\n",
       "total sulfur dioxide    0\n",
       "density                 0\n",
       "pH                      0\n",
       "sulphates               0\n",
       "alcohol                 0\n",
       "quality                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='quality')\n",
    "y = df['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1199, 11), (400, 11), (1199,), (400,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predication \n",
    "y_pred = svc.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.565\n",
      "precision score:  0.565\n",
      "recall score:  0.565\n",
      "f1 score:  0.565\n",
      "classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.00      0.00      0.00        13\n",
      "           5       0.61      0.76      0.68       164\n",
      "           6       0.52      0.60      0.56       169\n",
      "           7       0.00      0.00      0.00        48\n",
      "           8       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.56       400\n",
      "   macro avg       0.19      0.23      0.21       400\n",
      "weighted avg       0.47      0.56      0.51       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, f1_score, recall_score, precision_score, confusion_matrix, classification_report\n",
    "\n",
    "acc_score=accuracy_score(y_test,y_pred)\n",
    "pre_score=precision_score(y_test,y_pred, average='micro')\n",
    "rec_score=recall_score(y_test,y_pred, average='micro')\n",
    "F1_score=f1_score(y_test,y_pred, average='micro')\n",
    "report=classification_report(y_test,y_pred)\n",
    "print('accuracy score: ',acc_score)\n",
    "print('precision score: ',pre_score)\n",
    "print('recall score: ',rec_score)\n",
    "print('f1 score: ',F1_score)\n",
    "print('classification report: ',report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n",
      "[CV 1/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.542 total time=   0.0s\n",
      "[CV 2/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.588 total time=   0.0s\n",
      "[CV 4/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.592 total time=   0.0s\n",
      "[CV 5/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.542 total time=   0.0s\n",
      "[CV 2/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.588 total time=   0.0s\n",
      "[CV 4/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.592 total time=   0.0s\n",
      "[CV 5/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.542 total time=   0.0s\n",
      "[CV 2/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.588 total time=   0.0s\n",
      "[CV 4/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.592 total time=   0.0s\n",
      "[CV 5/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.542 total time=   0.0s\n",
      "[CV 2/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.588 total time=   0.0s\n",
      "[CV 4/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.592 total time=   0.0s\n",
      "[CV 5/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END C=0.1, gamma=0.0001, kernel=linear;, score=0.542 total time=   0.0s\n",
      "[CV 2/5] END C=0.1, gamma=0.0001, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END C=0.1, gamma=0.0001, kernel=linear;, score=0.588 total time=   0.0s\n",
      "[CV 4/5] END C=0.1, gamma=0.0001, kernel=linear;, score=0.592 total time=   0.0s\n",
      "[CV 5/5] END C=0.1, gamma=0.0001, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END .....C=0.5, gamma=1, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 2/5] END .....C=0.5, gamma=1, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END .....C=0.5, gamma=1, kernel=linear;, score=0.583 total time=   0.0s\n",
      "[CV 4/5] END .....C=0.5, gamma=1, kernel=linear;, score=0.588 total time=   0.0s\n",
      "[CV 5/5] END .....C=0.5, gamma=1, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END ...C=0.5, gamma=0.1, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 2/5] END ...C=0.5, gamma=0.1, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END ...C=0.5, gamma=0.1, kernel=linear;, score=0.583 total time=   0.0s\n",
      "[CV 4/5] END ...C=0.5, gamma=0.1, kernel=linear;, score=0.588 total time=   0.0s\n",
      "[CV 5/5] END ...C=0.5, gamma=0.1, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END ..C=0.5, gamma=0.01, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 2/5] END ..C=0.5, gamma=0.01, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END ..C=0.5, gamma=0.01, kernel=linear;, score=0.583 total time=   0.0s\n",
      "[CV 4/5] END ..C=0.5, gamma=0.01, kernel=linear;, score=0.588 total time=   0.0s\n",
      "[CV 5/5] END ..C=0.5, gamma=0.01, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END .C=0.5, gamma=0.001, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 2/5] END .C=0.5, gamma=0.001, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END .C=0.5, gamma=0.001, kernel=linear;, score=0.583 total time=   0.0s\n",
      "[CV 4/5] END .C=0.5, gamma=0.001, kernel=linear;, score=0.588 total time=   0.0s\n",
      "[CV 5/5] END .C=0.5, gamma=0.001, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, gamma=0.0001, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, gamma=0.0001, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, gamma=0.0001, kernel=linear;, score=0.583 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, gamma=0.0001, kernel=linear;, score=0.588 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, gamma=0.0001, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END .......C=1, gamma=1, kernel=linear;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END .......C=1, gamma=1, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END .......C=1, gamma=1, kernel=linear;, score=0.592 total time=   0.0s\n",
      "[CV 4/5] END .......C=1, gamma=1, kernel=linear;, score=0.583 total time=   0.0s\n",
      "[CV 5/5] END .......C=1, gamma=1, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.592 total time=   0.0s\n",
      "[CV 4/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.583 total time=   0.0s\n",
      "[CV 5/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.592 total time=   0.0s\n",
      "[CV 4/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.583 total time=   0.0s\n",
      "[CV 5/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.592 total time=   0.0s\n",
      "[CV 4/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.583 total time=   0.0s\n",
      "[CV 5/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END ..C=1, gamma=0.0001, kernel=linear;, score=0.550 total time=   0.0s\n",
      "[CV 2/5] END ..C=1, gamma=0.0001, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END ..C=1, gamma=0.0001, kernel=linear;, score=0.592 total time=   0.0s\n",
      "[CV 4/5] END ..C=1, gamma=0.0001, kernel=linear;, score=0.583 total time=   0.0s\n",
      "[CV 5/5] END ..C=1, gamma=0.0001, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END .......C=5, gamma=1, kernel=linear;, score=0.558 total time=   0.0s\n",
      "[CV 2/5] END .......C=5, gamma=1, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END .......C=5, gamma=1, kernel=linear;, score=0.583 total time=   0.0s\n",
      "[CV 4/5] END .......C=5, gamma=1, kernel=linear;, score=0.588 total time=   0.0s\n",
      "[CV 5/5] END .......C=5, gamma=1, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END .....C=5, gamma=0.1, kernel=linear;, score=0.558 total time=   0.0s\n",
      "[CV 2/5] END .....C=5, gamma=0.1, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END .....C=5, gamma=0.1, kernel=linear;, score=0.583 total time=   0.0s\n",
      "[CV 4/5] END .....C=5, gamma=0.1, kernel=linear;, score=0.588 total time=   0.0s\n",
      "[CV 5/5] END .....C=5, gamma=0.1, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END ....C=5, gamma=0.01, kernel=linear;, score=0.558 total time=   0.0s\n",
      "[CV 2/5] END ....C=5, gamma=0.01, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END ....C=5, gamma=0.01, kernel=linear;, score=0.583 total time=   0.0s\n",
      "[CV 4/5] END ....C=5, gamma=0.01, kernel=linear;, score=0.588 total time=   0.0s\n",
      "[CV 5/5] END ....C=5, gamma=0.01, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END ...C=5, gamma=0.001, kernel=linear;, score=0.558 total time=   0.0s\n",
      "[CV 2/5] END ...C=5, gamma=0.001, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END ...C=5, gamma=0.001, kernel=linear;, score=0.583 total time=   0.0s\n",
      "[CV 4/5] END ...C=5, gamma=0.001, kernel=linear;, score=0.588 total time=   0.0s\n",
      "[CV 5/5] END ...C=5, gamma=0.001, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END ..C=5, gamma=0.0001, kernel=linear;, score=0.558 total time=   0.0s\n",
      "[CV 2/5] END ..C=5, gamma=0.0001, kernel=linear;, score=0.537 total time=   0.0s\n",
      "[CV 3/5] END ..C=5, gamma=0.0001, kernel=linear;, score=0.583 total time=   0.0s\n",
      "[CV 4/5] END ..C=5, gamma=0.0001, kernel=linear;, score=0.588 total time=   0.0s\n",
      "[CV 5/5] END ..C=5, gamma=0.0001, kernel=linear;, score=0.657 total time=   0.0s\n",
      "[CV 1/5] END ......C=10, gamma=1, kernel=linear;, score=0.558 total time=   0.1s\n",
      "[CV 2/5] END ......C=10, gamma=1, kernel=linear;, score=0.537 total time=   0.1s\n",
      "[CV 3/5] END ......C=10, gamma=1, kernel=linear;, score=0.588 total time=   0.1s\n",
      "[CV 4/5] END ......C=10, gamma=1, kernel=linear;, score=0.583 total time=   0.1s\n",
      "[CV 5/5] END ......C=10, gamma=1, kernel=linear;, score=0.657 total time=   0.1s\n",
      "[CV 1/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.558 total time=   0.1s\n",
      "[CV 2/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.537 total time=   0.1s\n",
      "[CV 3/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.588 total time=   0.1s\n",
      "[CV 4/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.583 total time=   0.1s\n",
      "[CV 5/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.657 total time=   0.1s\n",
      "[CV 1/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.558 total time=   0.1s\n",
      "[CV 2/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.537 total time=   0.1s\n",
      "[CV 3/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.588 total time=   0.1s\n",
      "[CV 4/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.583 total time=   0.1s\n",
      "[CV 5/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.657 total time=   0.1s\n",
      "[CV 1/5] END ..C=10, gamma=0.001, kernel=linear;, score=0.558 total time=   0.1s\n",
      "[CV 2/5] END ..C=10, gamma=0.001, kernel=linear;, score=0.537 total time=   0.1s\n",
      "[CV 3/5] END ..C=10, gamma=0.001, kernel=linear;, score=0.588 total time=   0.1s\n",
      "[CV 4/5] END ..C=10, gamma=0.001, kernel=linear;, score=0.583 total time=   0.1s\n",
      "[CV 5/5] END ..C=10, gamma=0.001, kernel=linear;, score=0.657 total time=   0.1s\n",
      "[CV 1/5] END .C=10, gamma=0.0001, kernel=linear;, score=0.558 total time=   0.1s\n",
      "[CV 2/5] END .C=10, gamma=0.0001, kernel=linear;, score=0.537 total time=   0.1s\n",
      "[CV 3/5] END .C=10, gamma=0.0001, kernel=linear;, score=0.588 total time=   0.1s\n",
      "[CV 4/5] END .C=10, gamma=0.0001, kernel=linear;, score=0.583 total time=   0.1s\n",
      "[CV 5/5] END .C=10, gamma=0.0001, kernel=linear;, score=0.657 total time=   0.1s\n",
      "[CV 1/5] END ......C=50, gamma=1, kernel=linear;, score=0.558 total time=   0.3s\n",
      "[CV 2/5] END ......C=50, gamma=1, kernel=linear;, score=0.537 total time=   0.3s\n",
      "[CV 3/5] END ......C=50, gamma=1, kernel=linear;, score=0.588 total time=   0.2s\n",
      "[CV 4/5] END ......C=50, gamma=1, kernel=linear;, score=0.583 total time=   0.3s\n",
      "[CV 5/5] END ......C=50, gamma=1, kernel=linear;, score=0.657 total time=   0.3s\n",
      "[CV 1/5] END ....C=50, gamma=0.1, kernel=linear;, score=0.558 total time=   0.3s\n",
      "[CV 2/5] END ....C=50, gamma=0.1, kernel=linear;, score=0.537 total time=   0.3s\n",
      "[CV 3/5] END ....C=50, gamma=0.1, kernel=linear;, score=0.588 total time=   0.2s\n",
      "[CV 4/5] END ....C=50, gamma=0.1, kernel=linear;, score=0.583 total time=   0.3s\n",
      "[CV 5/5] END ....C=50, gamma=0.1, kernel=linear;, score=0.657 total time=   0.3s\n",
      "[CV 1/5] END ...C=50, gamma=0.01, kernel=linear;, score=0.558 total time=   0.3s\n",
      "[CV 2/5] END ...C=50, gamma=0.01, kernel=linear;, score=0.537 total time=   0.3s\n",
      "[CV 3/5] END ...C=50, gamma=0.01, kernel=linear;, score=0.588 total time=   0.2s\n",
      "[CV 4/5] END ...C=50, gamma=0.01, kernel=linear;, score=0.583 total time=   0.3s\n",
      "[CV 5/5] END ...C=50, gamma=0.01, kernel=linear;, score=0.657 total time=   0.3s\n",
      "[CV 1/5] END ..C=50, gamma=0.001, kernel=linear;, score=0.558 total time=   0.3s\n",
      "[CV 2/5] END ..C=50, gamma=0.001, kernel=linear;, score=0.537 total time=   0.3s\n",
      "[CV 3/5] END ..C=50, gamma=0.001, kernel=linear;, score=0.588 total time=   0.2s\n",
      "[CV 4/5] END ..C=50, gamma=0.001, kernel=linear;, score=0.583 total time=   0.3s\n",
      "[CV 5/5] END ..C=50, gamma=0.001, kernel=linear;, score=0.657 total time=   0.3s\n",
      "[CV 1/5] END .C=50, gamma=0.0001, kernel=linear;, score=0.558 total time=   0.3s\n",
      "[CV 2/5] END .C=50, gamma=0.0001, kernel=linear;, score=0.537 total time=   0.3s\n",
      "[CV 3/5] END .C=50, gamma=0.0001, kernel=linear;, score=0.588 total time=   0.2s\n",
      "[CV 4/5] END .C=50, gamma=0.0001, kernel=linear;, score=0.583 total time=   0.3s\n",
      "[CV 5/5] END .C=50, gamma=0.0001, kernel=linear;, score=0.657 total time=   0.3s\n",
      "[CV 1/5] END .....C=100, gamma=1, kernel=linear;, score=0.558 total time=   0.5s\n",
      "[CV 2/5] END .....C=100, gamma=1, kernel=linear;, score=0.537 total time=   0.6s\n",
      "[CV 3/5] END .....C=100, gamma=1, kernel=linear;, score=0.588 total time=   0.6s\n",
      "[CV 4/5] END .....C=100, gamma=1, kernel=linear;, score=0.588 total time=   0.5s\n",
      "[CV 5/5] END .....C=100, gamma=1, kernel=linear;, score=0.657 total time=   0.5s\n",
      "[CV 1/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.558 total time=   0.5s\n",
      "[CV 2/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.537 total time=   0.5s\n",
      "[CV 3/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.588 total time=   0.5s\n",
      "[CV 4/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.588 total time=   0.5s\n",
      "[CV 5/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.657 total time=   0.5s\n",
      "[CV 1/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.558 total time=   0.5s\n",
      "[CV 2/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.537 total time=   0.5s\n",
      "[CV 3/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.588 total time=   0.6s\n",
      "[CV 4/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.588 total time=   0.5s\n",
      "[CV 5/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.657 total time=   0.5s\n",
      "[CV 1/5] END .C=100, gamma=0.001, kernel=linear;, score=0.558 total time=   0.5s\n",
      "[CV 2/5] END .C=100, gamma=0.001, kernel=linear;, score=0.537 total time=   0.5s\n",
      "[CV 3/5] END .C=100, gamma=0.001, kernel=linear;, score=0.588 total time=   0.6s\n",
      "[CV 4/5] END .C=100, gamma=0.001, kernel=linear;, score=0.588 total time=   0.5s\n",
      "[CV 5/5] END .C=100, gamma=0.001, kernel=linear;, score=0.657 total time=   0.5s\n",
      "[CV 1/5] END C=100, gamma=0.0001, kernel=linear;, score=0.558 total time=   0.5s\n",
      "[CV 2/5] END C=100, gamma=0.0001, kernel=linear;, score=0.537 total time=   0.5s\n",
      "[CV 3/5] END C=100, gamma=0.0001, kernel=linear;, score=0.588 total time=   0.6s\n",
      "[CV 4/5] END C=100, gamma=0.0001, kernel=linear;, score=0.588 total time=   0.5s\n",
      "[CV 5/5] END C=100, gamma=0.0001, kernel=linear;, score=0.657 total time=   0.5s\n",
      "best parameters:  SVC(C=100, gamma=1, kernel='linear')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=100, gamma=1, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=100, gamma=1, kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=100, gamma=1, kernel='linear')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Tune the hyperparameters using GridSearchCV\n",
    "#defining parameter range\n",
    "\n",
    "param_grid = { 'C' :[0.1, 0.5, 1, 5, 10, 50, 100],\n",
    "              'gamma' : [1,0.1,0.01,0.001,0.0001],\n",
    "              'kernel' : ['linear']\n",
    "\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, verbose=3, refit=True)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best hyperparameters and train the classifier on the entire dataset\n",
    "best_classifier = grid_search.best_estimator_\n",
    "print(\"best parameters: \", best_classifier)\n",
    "best_classifier.fit(X_train_scaled, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5625\n"
     ]
    }
   ],
   "source": [
    "#trainig the entire dataset with best parameters\n",
    "svc = SVC(kernel='linear', C=100, gamma=1)\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# predication \n",
    "y_pred4=grid_search.predict(X_test_scaled)\n",
    "print(accuracy_score(y_test, y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_classifier.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(best_classifier, 'svm_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
