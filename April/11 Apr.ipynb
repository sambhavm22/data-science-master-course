{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?\n",
    "\n",
    "Q2. Why are ensemble techniques used in machine learning?\n",
    "\n",
    "Q3. What is bagging?\n",
    "\n",
    "Q4. What is boosting?\n",
    "\n",
    "Q5. What are the benefits of using ensemble techniques?\n",
    "\n",
    "Q6. Are ensemble techniques always better than individual models?\n",
    "\n",
    "Q7. How is the confidence interval calculated using bootstrap?\n",
    "\n",
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
    "\n",
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ensemble technique in machine learning refers to a methodology where multiple individual models, often referred to as \"base models\" or \"weak earners\", are combined to form a more powerful and accurate predictive model. The main idea behind ensemble techniques is that the combination of multiple models can often lead to better performance than any single model alone.\n",
    "\n",
    "Ensemble techniques can be applied to various types of machine learning algorithms, such as decision trees, neural networks, support vector machines, and more. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Why are ensemble techniques used in machine learning?\n",
    "\n",
    "### &\n",
    "\n",
    "### Q5. What are the benefits of using ensemble techniques?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Improved Accuracy: Ensemble techniques can significantly improve the accuracy of predictive models compared to using a single model. By combining multiple models that may have different strengths and weaknesses, ensemble techniques can leverage the collective knowledge of the individual models and produce more accurate predictions.\n",
    "\n",
    "- Reduced Overfitting: Ensemble techniques help mitigate overfitting, which occurs when a model performs well on the training data but poorly on new, unseen data. Ensemble methods, such as bagging and boosting, introduce randomness or focus on misclassified instances during training, which helps reduce overfitting and improve generalization to unseen data.\n",
    "\n",
    "- Increased Robustness: Ensemble techniques enhance the robustness of machine learning models by reducing the impact of outliers or noisy data. Since the models in an ensemble are trained independently or sequentially, the ensemble can average out errors or biases, leading to more robust predictions.\n",
    "\n",
    "- Capturing Complex Relationships: Ensemble techniques can capture complex relationships present in the data by combining the diverse perspectives of individual models. Each model may focus on different subsets or aspects of the data, allowing the ensemble to collectively capture a broader range of patterns and make more accurate predictions.\n",
    "\n",
    "- Model Aggregation and Consensus: Ensemble techniques enable model aggregation and consensus through techniques such as averaging or voting. By combining the predictions of multiple models, the ensemble can provide a more reliable and stable prediction that represents a consensus among the individual models.\n",
    "\n",
    "- Flexibility and Versatility: Ensemble techniques can be applied to various machine learning algorithms, allowing flexibility in model selection. They are not limited to specific algorithms and can be used with decision trees, neural networks, support vector machines, and more.\n",
    "\n",
    "- Handling Different Data Characteristics: Different models may perform better on specific types of data or in different regions of the feature space. Ensemble techniques allow the combination of models with complementary strengths, enabling better performance across diverse data characteristics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. What is bagging?\n",
    "### &\n",
    "### Q4. What is boosting?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bagging (Bootstrap Aggregating): In bagging, multiple base models are trained independently on different subsets of the training data, which are randomly sampled with replacement. Each base model then provides a prediction, and the final prediction is obtained through a combination of these individual predictions, such as averaging or voting.\n",
    "\n",
    "Random Forest is a popular ensemble method that utilizes bagging, where decision trees are the base models. Each decision tree is trained on a different subset of the data, and the final prediction is obtained by averaging or voting the predictions from all the trees.\n",
    "\n",
    "2. Boosting: In boosting, base models are trained sequentially, where each subsequent model focuses on improving the performance of the previous models. At each iteration, the algorithm assigns higher weights to misclassified instances, making the subsequent model prioritize these instances and try to correctly classify them. The final prediction is a weighted combination of all the individual model predictions.\n",
    "\n",
    "AdaBoost (Adaptive Boosting) and Gradient Boosting are popular boosting algorithms used in ensemble techniques. AdaBoost assigns weights to each training instance, while Gradient Boosting builds subsequent models by minimizing the errors of the previous models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Are ensemble techniques always better than individual models?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While ensemble techniques have numerous benefits and often outperform individual models, they are not always guaranteed to be better in every scenario. The effectiveness of ensemble techniques depends on various factors, including the specific problem, the quality of individual models, and the diversity among the models within the ensemble. Here are a few considerations:\n",
    "\n",
    "1. Quality of Individual Models: The performance of an ensemble heavily relies on the quality and diversity of the individual models. If the base models are weak or highly correlated, the ensemble may not yield significant improvements over a single well-performing model.\n",
    "\n",
    "2. Data Availability: Ensemble techniques generally require a sufficient amount of diverse data to train multiple models effectively. If the dataset is limited or lacks diversity, it may be challenging to build a diverse ensemble that outperforms individual models.\n",
    "\n",
    "3. Computational Complexity: Ensemble techniques involve training and combining multiple models, which can be computationally intensive and time-consuming.\n",
    "\n",
    "4. Interpretability and Simplicity: Ensemble techniques often result in more complex models compared to individual models. If interpretability or simplicity is a critical requirement, using a single model may be preferred over an ensemble.\n",
    "\n",
    "5. Domain-specific Considerations: Different problem domains may exhibit unique characteristics that impact the effectiveness of ensemble techniques. It's important to consider domain-specific factors, such as the nature of the data, feature space, and noise levels, when deciding whether to use ensemble techniques.\n",
    "\n",
    "6. Trade-offs and Risks: Ensemble techniques may introduce additional risks, such as overfitting if not properly implemented. There is also the potential for increased complexity, maintenance, and model integration challenges when deploying ensemble models in production systems.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. How is the confidence interval calculated using bootstrap?\n",
    "\n",
    "### & \n",
    "\n",
    "### Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique used to estimate the sampling distribution of a statistic or to assess the uncertainty associated with a sample. It allows us to make inferences about the population based on the available data without assuming any specific distribution.\n",
    "\n",
    "Here are the steps involved in the bootstrap method:\n",
    "\n",
    "Original Sample: Start with the original dataset, which contains a set of observations or data points.\n",
    "\n",
    "Resampling: Randomly draw a sample (bootstrap sample) from the original dataset with replacement. The size of the bootstrap sample is the same as the original dataset, but it may contain duplicate instances and may omit some original instances. The process of sampling with replacement ensures that each instance in the original dataset has an equal chance of being selected in each bootstrap sample, allowing for variability in the resampled data.\n",
    "\n",
    "Statistic Calculation: Calculate the desired statistic of interest (e.g., mean, median, standard deviation, correlation, etc.) using the resampled data. This statistic represents the value obtained from the bootstrap sample.\n",
    "\n",
    "Repeat Resampling and Statistic Calculation: Repeat steps 2 and 3 a large number of times (typically thousands of iterations) to generate multiple bootstrap samples and calculate the statistic for each sample.\n",
    "\n",
    "Distribution of the Statistic: The collection of calculated statistics from the bootstrap samples forms the empirical sampling distribution of the statistic. This distribution represents the variability and uncertainty associated with the statistic based on the available data.\n",
    "\n",
    "Inference and Confidence Intervals: From the empirical sampling distribution, one can make various inferences, such as estimating the population parameter, testing hypotheses, or constructing confidence intervals. Confidence intervals are often estimated using the percentile method, where the lower and upper bounds of the interval correspond to specific percentiles of the empirical distribution (e.g., the 2.5th and 97.5th percentiles for a 95% confidence interval). Other methods, such as bias correction and acceleration, can also be used to improve the accuracy of the confidence interval estimation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To estimate the 95% confidence interval for the population mean height using the bootstrap method, you can follow these steps based on the given information:\n",
    "\n",
    "1. Original Sample: Start with the sample of 50 tree heights and the provided mean height of 15 meters and standard deviation of 2 meters.\n",
    "\n",
    "2. Bootstrap Resampling: Randomly select a sample (with replacement) from the original sample. The bootstrap sample size should be the same as the original sample size (50 in this case).\n",
    "\n",
    "3. Sample Statistic Calculation: Calculate the mean height for the bootstrap sample.\n",
    "\n",
    "4. Repeat Resampling and Statistic Calculation: Repeat steps 2 and 3 a large number of times (e.g., 1,000 or 10,000 iterations) to generate multiple bootstrap samples and calculate the mean height for each sample.\n",
    "\n",
    "5. Bootstrap Distribution: Collect the mean height values obtained from the bootstrap samples to form the bootstrap distribution of the sample mean.\n",
    "\n",
    "6. Confidence Interval Estimation: From the bootstrap distribution, estimate the 95% confidence interval for the population mean height. The confidence interval is typically estimated using the percentile method.\n",
    "\n",
    "- Here's how you can calculate the 95% confidence interval using the bootstrap method for the given data:\n",
    "\n",
    "1. Generate a large number of bootstrap samples (e.g., 10,000 iterations) by randomly selecting 50 heights with replacement from the original sample.\n",
    "\n",
    "2. For each bootstrap sample, calculate the mean height.\n",
    "\n",
    "3. Collect all the calculated mean heights to form the bootstrap distribution.\n",
    "\n",
    "4. Sort the bootstrap distribution in ascending order.\n",
    "\n",
    "5. The lower bound of the 95% confidence interval corresponds to the 2.5th percentile of the bootstrap distribution.\n",
    "\n",
    "6. The upper bound of the 95% confidence interval corresponds to the 97.5th percentile of the bootstrap distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for Population Mean Height:\n",
      "Lower Bound: 5.80\n",
      "Upper Bound: 8.18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#original sample data\n",
    "sample_size = 50\n",
    "sample_mean = 15 \n",
    "sample_std = 2\n",
    "\n",
    "#initiating no of bootstrap iterations\n",
    "num_of_iterations = 1000\n",
    "\n",
    "# generating bootstrap samples\n",
    "bootstrap_means = []\n",
    "\n",
    "for i in range(num_of_iterations):\n",
    "    bootstrap_sample = np.random.choice(sample_mean, size=sample_size, replace=True)\n",
    "\n",
    "    # calculating mean of bootstrap sample\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "# calculating lower and upper bounds of confidence interval\n",
    "\n",
    "lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "upper_bound = np.percentile(bootstrap_means, 97.5)    \n",
    "\n",
    "# Print the confidence interval\n",
    "print(\"95% Confidence Interval for Population Mean Height:\")\n",
    "print(f\"Lower Bound: {lower_bound:.2f}\")\n",
    "print(f\"Upper Bound: {upper_bound:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
