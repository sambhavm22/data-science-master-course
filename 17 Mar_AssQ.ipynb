{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43962f6f-325a-454f-9b02-bcc9df55f90b",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n",
    "\n",
    "Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "\n",
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n",
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required.\n",
    "\n",
    "Q5: What is data Augmentation? Explain SMOTE.\n",
    "\n",
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n",
    "\n",
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "\n",
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n",
    "\n",
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6535b55e-77ab-48c8-bb25-e0ae79e7ce64",
   "metadata": {},
   "source": [
    "### Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc64d3a-e1f3-460f-9bdd-24487128617b",
   "metadata": {},
   "source": [
    "\n",
    "Missing values in a dataset are values that are not present for one or more variables or observations. Missing values can occur for a variety of reasons such as data collection errors, non-response, or data preprocessing issues.\n",
    "\n",
    "It is essential to haPrediction models: This method involves using machine learning algorithms to predict the missing values in a dataset based on the available data.ndle missing values in a dataset because they can have a significant impact on the results of data analysis, modeling, and machine learning algorithms. If not handled properly, missing values can result in biased or inaccurate results, reduced statistical power, and potentially incorrect conclusions.\n",
    "\n",
    "Some algorithms that are not affected by missing values include decision trees, random forests, and gradient boosting algorithms. These algorithms can handle missing values by simply ignoring the missing values and splitting the data based on the available values. Other algorithms like k-nearest neighbors and support vector machines may require imputation or data preprocessing techniques to handle missing values properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c82ebfe-7dd0-4d9e-bc31-4e9693f735b8",
   "metadata": {},
   "source": [
    "### Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba1347-531c-4dcc-a140-4068298669e6",
   "metadata": {},
   "source": [
    "1) Deletion: Deleting the missing values from the dataset is the simplest method, but it can result in a loss of information. There are two methods for deletion:\n",
    "\n",
    "a) Listwise deletion: In this method, the entire row containing a missing value is removed from the dataset.\n",
    "\n",
    "b) Pairwise deletion: In this method, only the missing values in a particular variable are removed, and the rest of the data is used for analysis.\n",
    "\n",
    "2) Imputation: This method involves replacing missing values with a value that represents the best guess of what the missing value should be. There are different techniques for imputation such as mean imputation, median imputation, and mode imputation.\n",
    "\n",
    "3) Interpolation: This method involves estimating missing values by using a function that fits the non-missing values and then extrapolates the missing values using that function.\n",
    "\n",
    "4) Prediction models: This method involves using machine learning algorithms to predict the missing values in a dataset based on the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286f05ad-1293-44fa-b4a1-49177209537e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B     C\n",
       "0  1.0  5.0   9.0\n",
       "1  2.0  NaN  10.0\n",
       "2  NaN  7.0  11.0\n",
       "3  4.0  8.0   NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# create a sample dataset with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4],\n",
    "                   'B': [5, np.nan, 7, 8],\n",
    "                   'C': [9, 10, 11, np.nan]})\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24ec9454-5151-41a6-bddd-3863bee3e16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A    B     C\n",
      "0  1.000000  5.0   9.0\n",
      "1  2.000000  NaN  10.0\n",
      "2  2.333333  7.0  11.0\n",
      "3  4.000000  8.0   NaN\n"
     ]
    }
   ],
   "source": [
    "# impute the missing values in column 'A' using mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['A'] = imputer.fit_transform(df[['A']])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1138f27f-149c-49e8-ba9d-38d535cb5650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B     C\n",
      "0  1.0  5.0   9.0\n",
      "1  2.0  NaN  10.0\n",
      "2  NaN  7.0  11.0\n",
      "3  4.0  8.0  11.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a sample dataset with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4],\n",
    "                   'B': [5, np.nan, 7, 8],\n",
    "                   'C': [9, 10, 11, np.nan]})\n",
    "\n",
    "# interpolate the missing values in column 'C' using linear interpolation\n",
    "df['C'] = df['C'].interpolate()\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "860e8d71-14d4-417e-b6f2-47b5a78aa99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create a sample dataset with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4],\n",
    "                   'B': [5, np.nan, 7, 8],\n",
    "                   'C': [9, 10, 11, np.nan],\n",
    "                   'D': [12, 13, 14, 15]})\n",
    "\n",
    "# split the data into training and testing datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72980bbf-5565-4842-9e65-c628a6bb2839",
   "metadata": {},
   "source": [
    "### Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22db490d-a3ed-4daf-b901-efc3d185769c",
   "metadata": {},
   "source": [
    "Imbalanced data is a type of dataset where the number of instances in one class is significantly higher or lower than the other classes. For example, in a dataset with binary classification where one class represents only 10% of the dataset, and the other represents 90%, the dataset is imbalanced.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to biased model performance. The learning algorithms are designed to minimize the overall error rate, which means that they will focus on the majority class and may ignore the minority class. As a result, the model will perform poorly in predicting the minority class, which is often the class of interest. This can be a significant problem in many real-world applications such as fraud detection, disease diagnosis, and credit risk analysis, where the minority class is critical.\n",
    "\n",
    "For example, let's say we have a dataset with 1000 instances and two classes, where one class represents 90% of the dataset, and the other class represents 10%. If we build a model without handling the imbalanced data, the model will likely predict the majority class for most instances, resulting in a high accuracy rate. However, the model's performance in predicting the minority class will be poor, which is often the more critical class in real-world applications. This could result in severe consequences such as missing fraudulent transactions or misdiagnosing a disease.\n",
    "\n",
    "Therefore, it is essential to handle imbalanced data to ensure that the model's performance is not biased towards the majority class and to ensure that the model performs well on the minority class as well. There are several techniques used to handle imbalanced data, such as resampling, generating synthetic samples, and using cost-sensitive learning algorithms.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c04b1-1b67-4456-a2fb-cd262e0a6a34",
   "metadata": {},
   "source": [
    "### Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068980d8-4ca4-4e2e-9b5a-999aed578b97",
   "metadata": {},
   "source": [
    "Up-sampling involves increasing the number of instances in the minority class to balance the dataset with the majority class. This can be done by replicating existing instances or generating new synthetic instances using techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "\n",
    "Down-sampling involves decreasing the number of instances in the majority class to balance the dataset with the minority class. This can be done by randomly selecting a subset of instances from the majority class.\n",
    "\n",
    "For example, consider a dataset for a binary classification problem where one class represents 10% of the data, and the other class represents 90%. If we are interested in accurately predicting the minority class, we may use up-sampling to increase the number of instances in the minority class. On the other hand, if we are interested in building a model that can accurately predict both classes, we may use down-sampling to decrease the number of instances in the majority class.\n",
    "\n",
    "Another scenario where up-sampling may be required is when the minority class is critical, such as in fraud detection or disease diagnosis, and misclassifying the minority class can have severe consequences. In such cases, we may use up-sampling to ensure that the model has enough data to learn from the minority class.\n",
    "\n",
    "Similarly, down-sampling may be required when the majority class is noisy or contains a significant number of outliers that can negatively affect the model's performance. In such cases, we may use down-sampling to reduce the impact of the majority class on the model's performance.\n",
    "\n",
    "Overall, choosing the right sampling technique depends on the specific problem and dataset, and it is crucial to carefully evaluate the performance of the model on both classes to ensure that the model is not biased towards one class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390233e-fa21-4391-b3fe-eeee7e195ee7",
   "metadata": {},
   "source": [
    "### Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0b9509-15f0-4ec8-925c-59ae594cc6e2",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used to increase the size of a dataset by creating new synthetic data from the existing data. The goal of data augmentation is to improve the performance of machine learning models by providing more examples for the model to learn from.\n",
    "\n",
    "One popular data augmentation technique used in handling imbalanced data is called SMOTE (Synthetic Minority Over-sampling Technique). SMOTE works by creating new synthetic samples of the minority class by interpolating between existing samples. Specifically, SMOTE selects two or more similar instances from the minority class and creates new synthetic instances by combining the feature vectors of the selected instances.\n",
    "\n",
    "Here's how SMOTE works in detail:\n",
    "\n",
    "For each instance in the minority class, SMOTE selects k nearest neighbors from the same class. The value of k is a hyperparameter that controls the level of oversampling.\n",
    "\n",
    "SMOTE then selects one of the k nearest neighbors randomly and computes the difference between the feature vectors of the selected instance and the neighbor.\n",
    "\n",
    "SMOTE multiplies the difference by a random number between 0 and 1 and adds it to the selected instance to create a new synthetic instance.\n",
    "\n",
    "SMOTE repeats steps 2-3 for each selected instance in the minority class until the desired level of oversampling is achieved.\n",
    "\n",
    "By creating new synthetic instances, SMOTE helps to balance the dataset and provide more examples for the model to learn from. SMOTE can also help to reduce the risk of overfitting and improve the generalization performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ccbd80-f51e-4998-8876-7cb67eb5da08",
   "metadata": {},
   "source": [
    "### Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f31b76-97a9-419f-93e3-0c4361f57031",
   "metadata": {},
   "source": [
    "Outliers are data points in a dataset that are significantly different from the other data points. Outliers can be caused by measurement errors, data entry errors, or extreme events that are rare but have a significant impact on the variable of interest.\n",
    "\n",
    "It's essential to handle outliers because they can significantly affect the statistical analysis and machine learning models built on the dataset. Outliers can skew the distribution of the data, making it difficult to estimate the parameters of the distribution accurately. Outliers can also have a disproportionate impact on the model's performance, leading to overfitting or underfitting of the model.\n",
    "\n",
    "Some reasons why it's essential to handle outliers:\n",
    "\n",
    "To improve accuracy,\n",
    "To improve the performance of machine learning models,\n",
    "To improve the interpretability of the results,\n",
    "To improve the validity of statistical tests\n",
    "\n",
    "There are several techniques for handling outliers, including:\n",
    "\n",
    "Removing outliers,\n",
    "Transforming the data,\n",
    "Winsorizing,\n",
    "Using robust statistical methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d160cdf-06ce-4b88-9d6c-6c492cfdd300",
   "metadata": {},
   "source": [
    "### Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15beec6a-aa85-432a-b797-00564375d047",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to handle missing data in an analysis. The appropriate technique depends on the amount of missing data, the nature of the missing data, and the specific problem at hand. Here are some techniques that can be used:\n",
    "\n",
    "Deletion: This involves removing the data points with missing values from the dataset. This can be done either by listwise deletion, which removes any data points with missing values from the analysis, or pairwise deletion, which removes only the data points with missing values from specific variables used in the analysis. However, this can lead to a loss of information and may bias the analysis if the missing data is not random.\n",
    "\n",
    "Imputation: This involves filling in the missing values with estimated values based on the available data. There are several imputation techniques that can be used, including mean imputation, regression imputation, and k-nearest neighbor imputation. Mean imputation involves replacing missing values with the mean of the available data, while regression imputation involves using a regression model to predict the missing values based on the other variables in the dataset. K-nearest neighbor imputation involves finding the k-nearest neighbors to the data point with missing values and using their values to estimate the missing values.\n",
    "\n",
    "Data augmentation: This involves creating new synthetic data points to replace the missing values. This can be done using techniques such as multiple imputation, which creates multiple imputed datasets by simulating plausible values for the missing data based on the observed data and combining the results.\n",
    "\n",
    "Advanced methods: There are several advanced techniques that can be used to handle missing data, including maximum likelihood estimation, expectation-maximization, and Bayesian methods. These techniques can be used when the missing data is not missing completely at random (MCAR) and requires more sophisticated modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a477e-fd51-40ba-b173-4d73169a6de2",
   "metadata": {},
   "source": [
    "### Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de1655a-b1a4-493f-8b3c-6d5424bfd0b2",
   "metadata": {},
   "source": [
    "Determining if missing data is missing at random or if there is a pattern to the missing data is important for selecting an appropriate strategy to handle the missing data. Here are some strategies that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data:\n",
    "\n",
    "Analyzing missing data patterns: One way to determine if there is a pattern to the missing data is to analyze the patterns of missing data across the variables in the dataset. This can be done by creating a missing data matrix or plot that shows the pattern of missing data across the variables. If the pattern of missing data is similar across variables, it may indicate that the missing data is not missing at random.\n",
    "\n",
    "Investigating the mechanism of missingness: Another way to determine if the missing data is missing at random or not is to investigate the mechanism of missingness. This involves understanding why the data is missing and whether there is a systematic reason for the missing data. For example, if the missing data is due to a technical problem with data collection, it may be missing at random. However, if the missing data is due to non-response or refusal to answer a question, it may not be missing at random.\n",
    "\n",
    "Using statistical tests: Statistical tests can also be used to determine if the missing data is missing at random or if there is a pattern to the missing data. One way to do this is to compare the characteristics of the missing data to the characteristics of the observed data. If there is no significant difference between the two, it may indicate that the missing data is missing at random.\n",
    "\n",
    "Imputation: Imputation techniques can also be used to determine if the missing data is missing at random or if there is a pattern to the missing data. Imputation techniques such as multiple imputation can be used to simulate the missing data and compare the results to the observed data. If the simulated data matches the observed data closely, it may indicate that the missing data is missing at random.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac0f961-a3ce-4fec-81c5-3a1d8a470b70",
   "metadata": {},
   "source": [
    "### Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b26f8bd-e0e8-464a-bd10-fedc5129933d",
   "metadata": {},
   "source": [
    "When working with an imbalanced dataset, where one class has significantly fewer samples than the other, it is important to evaluate the performance of the machine learning model carefully. Here are some methods: -\n",
    "\n",
    "Use appropriate evaluation metrics: Accuracy is not a reliable metric to evaluate the performance of a model on an imbalanced dataset, as it tends to favor the majority class. Instead, metrics such as precision, recall, F1 score, and area under the ROC curve (AUC-ROC) are more suitable for imbalanced datasets. These metrics take into account both the true positive rate and false positive rate, which are crucial for evaluating the performance of a model on imbalanced datasets.\n",
    "\n",
    "Resampling techniques: Resampling techniques can be used to balance the dataset by either oversampling the minority class or undersampling the majority class. Oversampling can be done by replicating the minority class samples, while undersampling can be done by removing some samples from the majority class. Some commonly used techniques for resampling are random undersampling, random oversampling, and Synthetic Minority Over-sampling Technique (SMOTE).\n",
    "\n",
    "Use of different algorithms: Some machine learning algorithms, such as decision trees and nearest neighbors, can perform well on imbalanced datasets, while others, such as logistic regression and support vector machines, can struggle. Therefore, trying out different algorithms can be helpful in finding the best performing one for the given dataset.\n",
    "\n",
    "Use of ensemble methods: Ensemble methods such as bagging, boosting, and stacking can be used to improve the performance of the model on the minority class. These methods combine the outputs of multiple models to create a stronger, more accurate model.\n",
    "\n",
    "Domain knowledge: Domain knowledge can be used to identify important features and variables that can help improve the performance of the model on the minority class. For example, in the medical diagnosis project, some features may be more important in predicting the minority class, and including them in the model can improve its performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22be0aad-291c-457d-be8f-e2dade871414",
   "metadata": {},
   "source": [
    "### Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc8bd4-fc6b-45e0-b4f9-0b30f8cfea78",
   "metadata": {},
   "source": [
    "To balance the dataset and down-sample the majority class, there are several methods that can be used. Here are some of the most commonly used techniques:\n",
    "\n",
    "Random under-sampling: This method involves randomly selecting a subset of samples from the majority class to match the number of samples in the minority class. This can be done using various sampling techniques, such as random sampling, cluster-based sampling, or instance hardness threshold.\n",
    "\n",
    "Cluster-based under-sampling: This method involves identifying clusters of samples from the majority class and keeping only one sample from each cluster. The remaining samples are discarded, thereby balancing the dataset.\n",
    "\n",
    "Instance hardness threshold: This method involves calculating the hardness of each sample in the majority class using a pre-defined threshold. The hardest samples are then removed, resulting in a balanced dataset.\n",
    "\n",
    "Tomek links: This method involves identifying pairs of samples from different classes that are the nearest neighbors of each other. The majority class sample is then removed, resulting in a balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f873ca-8aca-4653-9b3e-9a309ce0e52a",
   "metadata": {},
   "source": [
    "### Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d8cf9-fa77-40ac-bc58-09749f7dc6f2",
   "metadata": {},
   "source": [
    "To balance the dataset and up-sample the minority class, there are several methods that can be used. Here are some of the most commonly used techniques:\n",
    "\n",
    "Random over-sampling: This method involves randomly duplicating samples from the minority class to match the number of samples in the majority class. This can be done using various sampling techniques, such as random sampling, cluster-based sampling, or instance hardness threshold.\n",
    "\n",
    "SMOTE: Synthetic Minority Over-sampling Technique (SMOTE) is a popular over-sampling method that involves creating synthetic samples for the minority class by interpolating between neighboring samples. SMOTE generates new samples by randomly selecting a minority class sample and creating synthetic samples along the line segment joining the selected sample to its k nearest neighbors.\n",
    "\n",
    "ADASYN: Adaptive Synthetic Sampling (ADASYN) is a variant of SMOTE that generates synthetic samples using a weighted distribution, where the weight of each sample is inversely proportional to its density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c14596-8be5-4ad6-aed4-cde876c4cc00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
